{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(valid_r_preds[:, 1])\n",
    "#calculate the predictions of it being real for all, fake is the oppostion\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "#final_guesses[:len(valid_r)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1378 295 296\n",
      "909 195 195\n"
     ]
    }
   ],
   "source": [
    "#create data\n",
    "#good = np.loadtxt(\"clean_real.txt\", dtype = str)\n",
    "def make_data():\n",
    "    good = open(\"clean_real.txt\", \"r\")\n",
    "    words = good.read()\n",
    "    words = words.split(\"\\n\")\n",
    "    #print(len(words))\n",
    "    total = len(words)\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(words)\n",
    "    #print(words[0])\n",
    "    end1 = int(total*0.7)\n",
    "    end2 = int(total*0.85)\n",
    "    train = words[:end1]\n",
    "    valid = words[end1:end2]\n",
    "    test = words[end2:]\n",
    "    print(len(train), len(valid), len(test))\n",
    "\n",
    "    bad = open(\"clean_fake.txt\", \"r\")\n",
    "    words = bad.read()\n",
    "    words = words.split(\"\\n\")\n",
    "    #print(len(words))\n",
    "    total = len(words)\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(words)\n",
    "    #print(words[0])\n",
    "    end1 = int(total*0.7)\n",
    "    end2 = int(total*0.85)\n",
    "    train_bad = words[:end1]\n",
    "    valid_bad = words[end1:end2]\n",
    "    test_bad = words[end2:]\n",
    "    print(len(train_bad), len(valid_bad), len(test_bad))\n",
    "    \n",
    "    return train, valid, test, train_bad, valid_bad, test_bad\n",
    "p1, p2, p3, p4, p5, p6 = make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#p1.extend(p4)\n",
    "#p2.extend(p5)\n",
    "#p3.extend(p6)\n",
    "pickle.dump(p1, open(\"train_r.pkl\", \"wb\"))\n",
    "pickle.dump(p2, open(\"valid_r.pkl\", \"wb\"))\n",
    "pickle.dump(p3, open(\"test_r.pkl\", \"wb\"))\n",
    "pickle.dump(p4, open(\"train_b.pkl\", \"wb\"))\n",
    "pickle.dump(p5, open(\"valid_b.pkl\", \"wb\"))\n",
    "pickle.dump(p6, open(\"test_b.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the native Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_r = pickle.load(open(\"train_r.pkl\", \"rb\"))\n",
    "train_b = pickle.load(open(\"train_b.pkl\", \"rb\"))\n",
    "#test = pickle.load(open(\"test.pkl\", \"rb\"))#print(train)\n",
    "#valid = pickle.load(open(\"valid.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#EM\n",
    "#classes x dim(x) paramers for the models\n",
    "#pi_c os the probability it belongs to a certain class\n",
    "#(#classes -1) parametsr\n",
    "#if we assume gaussian model, there are 4 +1 params t estimate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2957 3019\n"
     ]
    }
   ],
   "source": [
    "#create features, which will be which words show up in each prase?\n",
    "#create dict  of keys to track how often the word occurs\n",
    "train_r_counts = {}\n",
    "for line in train_r:\n",
    "    words = line.split(\" \")\n",
    "    #print(words)\n",
    "    #break\n",
    "    for word in words:\n",
    "        try:\n",
    "            train_r_counts[word] +=1\n",
    "        except:\n",
    "            train_r_counts[word] =1\n",
    "\n",
    "train_b_counts = {}\n",
    "for line in train_b:\n",
    "    words = line.split(\" \")\n",
    "    #print(words)\n",
    "    #break\n",
    "    for word in words:\n",
    "        try:\n",
    "            train_b_counts[word] +=1\n",
    "        except:\n",
    "            train_b_counts[word] =1\n",
    "print(len(train_r_counts.keys()), len(train_b_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid_val(key_val, dict_list, p, m, total):\n",
    "    #m is the total unseen events added\n",
    "    #p is the prior\n",
    "    #aka delta smoothing\n",
    "    try:\n",
    "        val = dict_list[key_val]\n",
    "    except:\n",
    "        val = 0\n",
    "    final_val = (val+(m*p))/(total +m)\n",
    "    return final_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_counts[\"trump\"]\n",
    "total_real = len(train_r)\n",
    "total_fake = len(train_b)\n",
    "\n",
    "total_train = []\n",
    "total_train.extend(train_r)\n",
    "total_train.extend(train_b)\n",
    "\n",
    "p_real = total_real/(total_real +total_fake)\n",
    "p_fake = total_fake/(total_fake + total_real)\n",
    "valid_r = pickle.load(open(\"valid_r.pkl\", \"rb\"))\n",
    "valid_b = pickle.load(open(\"valid_b.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4819\n"
     ]
    }
   ],
   "source": [
    "#real_news_probs = np.zeros((len(valid_r), ))\n",
    "#create hashmap of probabilities?\n",
    "\n",
    "def generate_probs(total_train, delta = 0.01, corpus= 10000):\n",
    "    prob_real = {}\n",
    "    prob_fake = {}\n",
    "    for j, line in enumerate(total_train):\n",
    "        words = line.split(\" \")\n",
    "        vals_bad = np.zeros(len(words))\n",
    "        vals_good = np.zeros_like(vals_bad)\n",
    "        for i, word in enumerate(words):\n",
    "            #the prob of (keywords) given class label\n",
    "            val = valid_val(word, train_r_counts, delta, corpus, total_real) #train_r_counts[word]\n",
    "            val2 = valid_val(word, train_b_counts, delta, corpus, total_fake)    \n",
    "            try:\n",
    "                prob_real[word]\n",
    "            except:\n",
    "                prob_real[word] = val\n",
    "            try:\n",
    "                prob_fake[word]\n",
    "            except:\n",
    "                prob_fake[word] = val2\n",
    "\n",
    "        #if j %100 ==99:\n",
    "        #    print(\"Line:\", j)\n",
    "    unique_real = list(prob_real.keys())\n",
    "    unique_fake = list(prob_fake.keys())\n",
    "    unique_real.extend(unique_fake)\n",
    "    unique_keys = np.unique(unique_real)\n",
    "    print(len(unique_keys))\n",
    "    return prob_real, prob_fake, unique_keys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prob_real, prob_fake, unique_keys = generate_probs(total_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295\n",
      "490\n",
      "(490,)\n",
      "Real Valid DataSet\n",
      "P: 0.904353069878\n",
      "P: 0.920208478845\n",
      "P: 0.936317687702\n",
      "P: 0.990433810028\n",
      "P: 0.9494900087\n",
      "P: 0.901780865372\n",
      "P: 0.935582527012\n",
      "P: 0.933510959173\n",
      "P: 0.939548560908\n",
      "P: 0.959806288842\n",
      "P: 0.968289590974\n",
      "P: 0.982573760827\n",
      "P: 0.960291189999\n",
      "P: 0.879145985012\n",
      "P: 0.895394011175\n",
      "P: 0.95974618131\n",
      "P: 0.907643316216\n",
      "P: 0.909646533243\n",
      "P: 0.895893208404\n",
      "P: 0.913608745399\n",
      "P: 0.937487405548\n",
      "P: 0.975229759189\n",
      "P: 0.958929942142\n",
      "P: 0.902491896868\n",
      "P: 0.894767546873\n",
      "P: 0.936263865176\n",
      "P: 0.903927785302\n",
      "P: 0.963212742512\n",
      "P: 0.960173326925\n",
      "P: 0.839436543702\n",
      "P: 0.954881487697\n",
      "P: 0.972552420476\n",
      "P: 0.957149917955\n",
      "P: 0.912076403577\n",
      "P: 0.957642117984\n",
      "P: 0.988697182046\n",
      "P: 0.905791377622\n",
      "P: 0.843292865936\n",
      "P: 0.98232532192\n",
      "P: 0.918396117193\n",
      "P: 0.954647082187\n",
      "P: 0.914614421995\n",
      "P: 0.975843907012\n",
      "P: 0.97951866116\n",
      "P: 0.910474305403\n",
      "P: 0.869260851849\n",
      "P: 0.899762705669\n",
      "P: 0.935890276722\n",
      "P: 0.934371282654\n",
      "P: 0.964522641583\n",
      "P: 0.826649280777\n",
      "P: 0.97768047987\n",
      "P: 0.901729001109\n",
      "P: 0.898564674705\n",
      "P: 0.964647348408\n",
      "P: 0.978862724109\n",
      "P: 0.868574897074\n",
      "P: 0.911489677235\n",
      "P: 0.968652297318\n",
      "P: 0.902748843303\n",
      "P: 0.934473215066\n",
      "P: 0.896395368418\n",
      "P: 0.909855276683\n",
      "P: 0.958611281643\n",
      "P: 0.945505084008\n",
      "P: 0.797567957033\n",
      "P: 0.937663167767\n",
      "P: 0.979100869581\n",
      "P: 0.908173824598\n",
      "P: 0.917136510284\n",
      "P: 0.966541651377\n",
      "P: 0.937424333583\n",
      "P: 0.968433615099\n",
      "P: 0.968870311882\n",
      "P: 0.951714644813\n",
      "P: 0.829698477347\n",
      "P: 0.964988565916\n",
      "P: 0.975798637884\n",
      "P: 0.907953023358\n",
      "P: 0.954043942583\n",
      "P: 0.968607954815\n",
      "P: 0.957447734733\n",
      "P: 0.960792096408\n",
      "P: 0.904197663777\n",
      "P: 0.88688300624\n",
      "P: 0.893627798752\n",
      "P: 0.96256775112\n",
      "P: 0.956955610269\n",
      "P: 0.858827580632\n",
      "P: 0.925397927188\n",
      "P: 0.966587188506\n",
      "P: 0.860336034609\n",
      "P: 0.95504161397\n",
      "P: 0.943892370719\n",
      "P: 0.898997265408\n",
      "P: 0.864070749094\n",
      "P: 0.918188867203\n",
      "P: 0.936944590896\n",
      "P: 0.953913381393\n",
      "P: 0.956380189609\n",
      "P: 0.919856820819\n",
      "P: 0.962020417157\n",
      "P: 0.934216598297\n",
      "P: 0.948242917143\n",
      "P: 0.85239634514\n",
      "P: 0.983473043814\n",
      "P: 0.979243030843\n",
      "P: 0.92076666609\n",
      "P: 0.957589017836\n",
      "P: 0.960769704856\n",
      "P: 0.903036679103\n",
      "P: 0.972487293627\n",
      "P: 0.881796654719\n",
      "P: 0.954564874024\n",
      "P: 0.988926713091\n",
      "P: 0.962648620023\n",
      "P: 0.952395568242\n",
      "P: 0.967483586552\n",
      "P: 0.964137509808\n",
      "P: 0.961006661102\n",
      "P: 0.982132205187\n",
      "P: 0.91535241413\n",
      "P: 0.908993348548\n",
      "P: 0.963246669564\n",
      "P: 0.98774410119\n",
      "P: 0.882212819067\n",
      "P: 0.930851866\n",
      "P: 0.961189975923\n",
      "P: 0.893734112765\n",
      "P: 0.945596763242\n",
      "P: 0.957551837011\n",
      "P: 0.927445434599\n",
      "P: 0.911147538538\n",
      "P: 0.963119307458\n",
      "P: 0.953941485054\n",
      "P: 0.930701339705\n",
      "P: 0.981810766643\n",
      "P: 0.96375880208\n",
      "P: 0.920078877702\n",
      "P: 0.949121003394\n",
      "P: 0.923591496241\n",
      "P: 0.913626882546\n",
      "P: 0.934373274997\n",
      "P: 0.964766907702\n",
      "P: 0.953084147345\n",
      "P: 0.96781798347\n",
      "P: 0.963155302025\n",
      "P: 0.989154342827\n",
      "P: 0.897023619067\n",
      "P: 0.942687237733\n",
      "P: 0.965470285973\n",
      "P: 0.986930310348\n",
      "P: 0.95147820303\n",
      "P: 0.828473768822\n",
      "P: 0.910969178287\n",
      "P: 0.919489582442\n",
      "P: 0.922854152786\n",
      "P: 0.672830876232\n",
      "P: 0.967831405643\n",
      "P: 0.953335710999\n",
      "P: 0.836173862062\n",
      "P: 0.916531369824\n",
      "P: 0.914654622807\n",
      "P: 0.944016623852\n",
      "P: 0.966779109877\n",
      "P: 0.886373180961\n",
      "P: 0.93766670498\n",
      "P: 0.967513495721\n",
      "P: 0.959935503811\n",
      "P: 0.882828804971\n",
      "P: 0.815438262925\n",
      "P: 0.916776707369\n",
      "P: 0.977764695155\n",
      "P: 0.961064586951\n",
      "P: 0.933096972278\n",
      "P: 0.970102846466\n",
      "P: 0.915944147235\n",
      "P: 0.981406669203\n",
      "P: 0.978875950326\n",
      "P: 0.969211883379\n",
      "P: 0.909882859047\n",
      "P: 0.904985100762\n",
      "P: 0.899921423131\n",
      "P: 0.985271253963\n",
      "P: 0.908606641756\n",
      "P: 0.954993689578\n",
      "P: 0.928538988313\n",
      "P: 0.988920663628\n",
      "P: 0.96303944202\n",
      "P: 0.949674662422\n",
      "P: 0.960326267138\n",
      "P: 0.93814441154\n",
      "P: 0.957521085473\n",
      "P: 0.921505924018\n",
      "P: 0.966000264717\n",
      "P: 0.905294075701\n",
      "P: 0.869184277719\n",
      "P: 0.958746198671\n",
      "P: 0.906398467432\n",
      "P: 0.981548171736\n",
      "P: 0.979448686546\n",
      "P: 0.869627993641\n",
      "P: 0.94169262915\n",
      "P: 0.911139875649\n",
      "P: 0.930220422199\n",
      "P: 0.890798416223\n",
      "P: 0.946984258848\n",
      "P: 0.922953220425\n",
      "P: 0.984910480932\n",
      "P: 0.956185837975\n",
      "P: 0.984970037903\n",
      "P: 0.885568170239\n",
      "P: 0.958502520792\n",
      "P: 0.918642605719\n",
      "P: 0.91913900556\n",
      "P: 0.910963055158\n",
      "P: 0.950743200143\n",
      "P: 0.979223156538\n",
      "P: 0.90468979366\n",
      "P: 0.915727144865\n",
      "P: 0.896219109045\n",
      "P: 0.969279282828\n",
      "P: 0.965086846742\n",
      "P: 0.971435564013\n",
      "P: 0.988913508527\n",
      "P: 0.974370052571\n",
      "P: 0.956952783051\n",
      "P: 0.954473889884\n",
      "P: 0.976969097371\n",
      "P: 0.965413119511\n",
      "P: 0.953406715977\n",
      "P: 0.854434448461\n",
      "P: 0.966288437463\n",
      "P: 0.949205006003\n",
      "P: 0.964104508667\n",
      "P: 0.932266656624\n",
      "P: 0.967477284708\n",
      "P: 0.878041460209\n",
      "P: 0.88988825505\n",
      "P: 0.905802761195\n",
      "P: 0.95664287954\n",
      "P: 0.924584068506\n",
      "P: 0.956128841373\n",
      "P: 0.937317534693\n",
      "P: 0.958067775923\n",
      "P: 0.936583333982\n",
      "P: 0.956995395099\n",
      "P: 0.964220013479\n",
      "P: 0.839510476453\n",
      "P: 0.974884019491\n",
      "P: 0.966546974898\n",
      "P: 0.961729033534\n",
      "P: 0.963097684945\n",
      "P: 0.91498697696\n",
      "P: 0.963891629989\n",
      "P: 0.884552360977\n",
      "P: 0.97471244662\n",
      "P: 0.959976186143\n",
      "P: 0.962119637494\n",
      "P: 0.955877976482\n",
      "P: 0.961916909096\n",
      "P: 0.982260621763\n",
      "P: 0.973673787603\n",
      "P: 0.916405900351\n",
      "P: 0.953632715978\n",
      "P: 0.909825293051\n",
      "P: 0.919218529315\n",
      "P: 0.852277827455\n",
      "P: 0.908901505343\n",
      "P: 0.83240028658\n",
      "P: 0.91045277021\n",
      "P: 0.917495134899\n",
      "P: 0.964702023864\n",
      "P: 0.769358026432\n",
      "P: 0.970402220319\n",
      "P: 0.951318638157\n",
      "P: 0.970222900932\n",
      "P: 0.968708662352\n",
      "P: 0.933190761756\n",
      "P: 0.975471694558\n",
      "P: 0.914626319297\n",
      "P: 0.961967423996\n",
      "P: 0.980380896031\n",
      "P: 0.971046459381\n",
      "P: 0.90113833873\n",
      "P: 0.936568603455\n",
      "P: 0.918660205264\n",
      "P: 0.885737796478\n",
      "P: 0.973994412437\n",
      "P: 0.961099201639\n",
      "P: 0.966131524028\n",
      "P: 0.975210000378\n",
      "P: 0.94085205876\n",
      "P: 0.960253266392\n",
      "P: 0.986043399784\n",
      "P: 0.840871175412\n",
      "P: 0.93937955346\n",
      "P: 0.629338357111\n",
      "P: 0.88086370882\n",
      "P: 0.794740512266\n",
      "P: 0.916163935645\n",
      "P: 0.93837110716\n",
      "P: 0.902874148008\n",
      "P: 0.920718920743\n",
      "P: 0.570812426093\n",
      "P: 0.964211819338\n",
      "P: 0.621594229277\n",
      "P: 0.814806211164\n",
      "P: 0.851883038969\n",
      "P: 0.62846262448\n",
      "P: 0.807473643545\n",
      "P: 0.894559244184\n",
      "P: 0.920418929722\n",
      "P: 0.709820711409\n",
      "P: 0.935867033544\n",
      "P: 0.938220243587\n",
      "P: 0.94744553801\n",
      "P: 0.74779710044\n",
      "P: 0.655712327799\n",
      "P: 0.851559756156\n",
      "P: 0.881499504002\n",
      "P: 0.786267372672\n",
      "P: 0.663406085041\n",
      "P: 0.548525462961\n",
      "P: 0.715752338989\n",
      "P: 0.914388820626\n",
      "P: 0.881790132471\n",
      "P: 0.889745665681\n",
      "P: 0.920173331463\n",
      "P: 0.841985545069\n",
      "P: 0.851834281771\n",
      "P: 0.873617592988\n",
      "P: 0.737203700235\n",
      "P: 0.81059029217\n",
      "P: 0.809931624441\n",
      "P: 0.774366140937\n",
      "P: 0.813624021499\n",
      "P: 0.481112439227\n",
      "P: 0.746947224213\n",
      "P: 0.858044918455\n",
      "P: 0.80050564279\n",
      "P: 0.784728686326\n",
      "P: 0.820853618676\n",
      "P: 0.823454795219\n",
      "P: 0.890962211257\n",
      "P: 0.866956331874\n",
      "P: 0.857763177149\n",
      "P: 0.903984172069\n",
      "P: 0.945644000053\n",
      "P: 0.61920575696\n",
      "P: 0.891902369093\n",
      "P: 0.801527208286\n",
      "P: 0.873103453467\n",
      "P: 0.947402060208\n",
      "P: 0.885511919952\n",
      "P: 0.879359596909\n",
      "P: 0.870087254594\n",
      "P: 0.8320625926\n",
      "P: 0.759140548068\n",
      "P: 0.887839916535\n",
      "P: 0.923275618761\n",
      "P: 0.768657176092\n",
      "P: 0.877648720942\n",
      "P: 0.698872204804\n",
      "P: 0.928936566088\n",
      "P: 0.914232493837\n",
      "P: 0.676362036266\n",
      "P: 0.819016099878\n",
      "P: 0.90559710181\n",
      "P: 0.91699984917\n",
      "P: 0.907476211939\n",
      "P: 0.831431487327\n",
      "P: 0.852640278999\n",
      "P: 0.808666598212\n",
      "P: 0.896520625524\n",
      "P: 0.83271622552\n",
      "P: 0.751508408102\n",
      "P: 0.806020039669\n",
      "P: 0.766428661998\n",
      "P: 0.853032160276\n",
      "P: 0.906458954297\n",
      "P: 0.902103088301\n",
      "P: 0.88033000079\n",
      "P: 0.65542557456\n",
      "P: 0.615490641912\n",
      "P: 0.53247192435\n",
      "P: 0.757311504022\n",
      "P: 0.779188962185\n",
      "P: 0.545815880341\n",
      "P: 0.915851399808\n",
      "P: 0.681687713186\n",
      "P: 0.858549781733\n",
      "P: 0.809854941916\n",
      "P: 0.737683041795\n",
      "P: 0.725559357616\n",
      "P: 0.907298544448\n",
      "P: 0.787804841624\n",
      "P: 0.715440953923\n",
      "P: 0.892399677059\n",
      "P: 0.841127575593\n",
      "P: 0.954883098633\n",
      "P: 0.844441639872\n",
      "P: 0.793487175626\n",
      "P: 0.898762418316\n",
      "P: 0.868929685229\n",
      "P: 0.853458419593\n",
      "P: 0.821905707522\n",
      "P: 0.574788513251\n",
      "P: 0.437351813354\n",
      "P: 0.676240404843\n",
      "P: 0.891681065511\n",
      "P: 0.913000027449\n",
      "P: 0.822066460239\n",
      "P: 0.937016232303\n",
      "P: 0.715831730394\n",
      "P: 0.643630052348\n",
      "P: 0.740826600589\n",
      "P: 0.911185717091\n",
      "P: 0.914277275801\n",
      "P: 0.873925460291\n",
      "P: 0.873013850874\n",
      "P: 0.565198314229\n",
      "P: 0.842696813283\n",
      "P: 0.761083638456\n",
      "P: 0.773498485555\n",
      "P: 0.682108853566\n",
      "P: 0.836120437008\n",
      "P: 0.838091451387\n",
      "P: 0.67870753266\n",
      "P: 0.804312155474\n",
      "P: 0.957739642831\n",
      "P: 0.906687842677\n",
      "P: 0.877159170367\n",
      "P: 0.726281514224\n",
      "P: 0.6613284316\n",
      "P: 0.810259564397\n",
      "P: 0.869457581952\n",
      "P: 0.656786216949\n",
      "P: 0.928452562623\n",
      "P: 0.797615700539\n",
      "P: 0.785671285729\n",
      "P: 0.845277364272\n",
      "P: 0.633683448313\n",
      "P: 0.906790556745\n",
      "P: 0.85888607736\n",
      "P: 0.86933890231\n",
      "P: 0.662055628951\n",
      "P: 0.863977883712\n",
      "P: 0.674419551049\n",
      "P: 0.668663253683\n",
      "P: 0.790926855651\n",
      "P: 0.938938493165\n",
      "P: 0.873122880908\n",
      "P: 0.76806141418\n",
      "P: 0.909304174206\n",
      "P: 0.694291981573\n",
      "P: 0.724803364701\n",
      "P: 0.888164789515\n",
      "P: 0.937023898561\n",
      "P: 0.694553152795\n",
      "P: 0.817273626859\n",
      "P: 0.54309161205\n",
      "P: 0.918631463638\n",
      "P: 0.671110651666\n",
      "P: 0.938235793297\n",
      "P: 0.686514536001\n",
      "P: 0.872709796809\n",
      "P: 0.675464300411\n",
      "P: 0.774848006032\n",
      "P: 0.734786441625\n",
      "P: 0.888742756316\n",
      "P: 0.8699763263\n",
      "P: 0.903340736191\n",
      "P: 0.877183750576\n",
      "P: 0.878553216086\n",
      "P: 0.947042886019\n",
      "P: 0.842540740142\n",
      "P: 0.93930321263\n",
      "P: 0.706781313921\n",
      "P: 0.803842279843\n",
      "P: 0.815286908771\n",
      "P: 0.91332770228\n",
      "P: 0.901159522135\n",
      "P: 0.758800065999\n",
      "P: 0.836144193145\n",
      "P: 0.906560534415\n",
      "P: 0.872564391045\n",
      "P: 0.857994234688\n",
      "P: 0.699851655303\n",
      "P: 0.869759964608\n"
     ]
    }
   ],
   "source": [
    "#now generate probabilities\n",
    "print(len(valid_r))\n",
    "\n",
    "total_valids = []\n",
    "total_valids.extend(valid_r)\n",
    "total_valids.extend(valid_b)\n",
    "\n",
    "def generate_guesses(total_valids, unique_keys, prob_real, prob_fake):\n",
    "    print(len(total_valids))\n",
    "    final_guesses = np.zeros(len(total_valids))\n",
    "    print(final_guesses.shape)\n",
    "    print(\"Real Valid DataSet\")\n",
    "    for i, line in enumerate(total_valids):\n",
    "        #if i == len(valid_r):\n",
    "        #    print(\"\\nFake Valid Data set\\n\")\n",
    "        words = line.split(\" \")\n",
    "        real_preds = np.zeros(len(unique_keys))\n",
    "        fake_preds   = np.zeros_like(real_preds)\n",
    "        for j, word in enumerate(unique_keys):\n",
    "            #prob assuming its real\n",
    "            if word in words:\n",
    "                real_preds[j] = prob_real[word]\n",
    "            else:\n",
    "                real_preds[j] = 1 - prob_real[word]\n",
    "            #prob assuming its false\n",
    "            if word in words:\n",
    "                fake_preds[j] = prob_fake[word]\n",
    "            else:\n",
    "                fake_preds[j] = 1 - prob_fake[word]\n",
    "\n",
    "        #final prob that its real given these words\n",
    "        vals = np.exp(np.sum(np.log(real_preds), axis = 0)) * p_real\n",
    "        vals2 = np.exp(np.sum(np.log(fake_preds), axis= 0)) *p_fake\n",
    "        prob = vals/(vals+vals2)\n",
    "        #print(\"P:\", prob)\n",
    "        final_guesses[i] = prob\n",
    "    return final_guesses\n",
    "    \n",
    "final_guesses = generate_guesses(total_valids, unique_keys, prob_real, prob_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsMAAAHVCAYAAAAU6/ZZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQLfEMAAA3RLDAAB06/8D8bv0L2+MQ5QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feba2278240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot('111')\n",
    "bins = np.linspace(0, 1, 100)\n",
    "ax.hist(final_guesses[:len(valid_r)], bins = bins, label = \"Real\", alpha = 0.5)\n",
    "#plt.show()\n",
    "#bins = np.linspace(0.8, 1, 40)\n",
    "ax.hist(final_guesses[len(valid_r):], bins = bins, label = \"Fake\", alpha = 0.5, color = \"green\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:CSC401]",
   "language": "python",
   "name": "conda-env-CSC401-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
